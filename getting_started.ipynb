{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import ClassLabel, Dataset\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "test_size = 0.2\n",
    "num_classes = 2\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "set_class_weights = True\n",
    "model_version = 'xlm-roberta-base'\n",
    "\n",
    "data = pd.read_csv('data_sample/train.csv')\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['text'],\n",
    "    data['label'],\n",
    "    test_size=test_size,\n",
    "    random_state=42, \n",
    "    stratify=data['label']\n",
    ")\n",
    "\n",
    "train = Dataset.from_pandas(pd.DataFrame({'text': X_train, 'label': y_train}))\n",
    "test = Dataset.from_pandas(pd.DataFrame({'text': X_test, 'label': y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if set_class_weights:\n",
    "    class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "else:\n",
    "    class_weights = None\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_version)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_version, num_labels=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681782daa32b4573a87be34424a54892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1241a324bf90429fb932b99863afa471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_batch(batch):\n",
    "    tokens = tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "   # tokens['label'] = labels.str2int(batch['label'])\n",
    "    return tokens\n",
    "\n",
    "train_tokenized = train.map(tokenize_batch, batched=True, batch_size=len(train))\n",
    "test_tokenized = test.map(tokenize_batch, batched=True, batch_size=len(test))\n",
    "\n",
    "train_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"f1\": f1, \"accuracy\": acc}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "#   warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=1,\n",
    "    optim='adamw_torch',\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-5,\n",
    "    save_strategy='steps',\n",
    "    save_steps=50\n",
    ")\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        # compute custom loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_tokenized,         # training dataset\n",
    "    eval_dataset=test_tokenized,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc96c2a1f7a492ebc8985995f053efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284eac5decb24da49965e22f650e30b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6933073401451111, 'eval_f1': 0.5184592145015106, 'eval_accuracy': 0.655, 'eval_runtime': 0.9216, 'eval_samples_per_second': 217.02, 'eval_steps_per_second': 14.106, 'epoch': 0.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34db69c7967941dfa7d266e5b7cf7b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6911154389381409, 'eval_f1': 0.5184592145015106, 'eval_accuracy': 0.655, 'eval_runtime': 0.9245, 'eval_samples_per_second': 216.335, 'eval_steps_per_second': 14.062, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872f00bebfad49988285a7a171b07aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6889051795005798, 'eval_f1': 0.3537420232241865, 'eval_accuracy': 0.42, 'eval_runtime': 0.9208, 'eval_samples_per_second': 217.191, 'eval_steps_per_second': 14.117, 'epoch': 1.2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb3b84a3428468c8248b11de07a5fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6089941263198853, 'eval_f1': 0.6778547451937748, 'eval_accuracy': 0.67, 'eval_runtime': 0.9274, 'eval_samples_per_second': 215.666, 'eval_steps_per_second': 14.018, 'epoch': 1.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8a7cd31b0749a59f2548d54355325e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6106681227684021, 'eval_f1': 0.7256923076923077, 'eval_accuracy': 0.73, 'eval_runtime': 0.9255, 'eval_samples_per_second': 216.093, 'eval_steps_per_second': 14.046, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d672f1ff001e4b95ad3426e41af44962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5805345177650452, 'eval_f1': 0.6776339285714286, 'eval_accuracy': 0.67, 'eval_runtime': 0.9215, 'eval_samples_per_second': 217.029, 'eval_steps_per_second': 14.107, 'epoch': 2.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9e6abaeab64d72822cde592eb0e4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6060137152671814, 'eval_f1': 0.7209230769230769, 'eval_accuracy': 0.72, 'eval_runtime': 0.9202, 'eval_samples_per_second': 217.334, 'eval_steps_per_second': 14.127, 'epoch': 2.8}\n",
      "{'train_runtime': 44.6605, 'train_samples_per_second': 53.739, 'train_steps_per_second': 1.679, 'train_loss': 0.6070699055989583, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=0.6070699055989583, metrics={'train_runtime': 44.6605, 'train_samples_per_second': 53.739, 'train_steps_per_second': 1.679, 'train_loss': 0.6070699055989583, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hatespeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
