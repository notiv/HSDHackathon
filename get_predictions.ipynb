{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import ClassLabel, Dataset\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_classes = 2\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_version = 'xlm-roberta-large'\n",
    "\n",
    "data = pd.read_csv('../new_data.csv')\n",
    "test = Dataset.from_pandas(pd.DataFrame({'text': data[\"text\"], 'label': data[\"label\"]}))\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_version)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_version, num_labels=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(batch):\n",
    "    tokens = tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "   # tokens['label'] = labels.str2int(batch['label'])\n",
    "    return tokens\n",
    "\n",
    "test_tokenized = test.map(tokenize_batch, batched=True, batch_size=len(test))\n",
    "test_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "eval_dataloader = DataLoader(test_tokenized, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_load = [\"roberta_large_v1_0779.pth\"]\n",
    "\n",
    "for i, model_name in enumerate(models_to_load):\n",
    "    model.load_state_dict(torch.load(f\"models/{model_name}\"))\n",
    "\n",
    "    pred_probas = np.empty((0, 2))\n",
    "\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        y_pred_proba = torch.softmax(logits, axis=1).cpu().numpy()\n",
    "        pred_probas = np.vstack((pred_probas, y_pred_proba))\n",
    "\n",
    "    if i == 0:\n",
    "        all_preds = y_pred_proba\n",
    "    else:\n",
    "        all_preds += y_pred_proba"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
